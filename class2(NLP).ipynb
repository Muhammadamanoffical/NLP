{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5700f7e2-78f5-43ef-a9b9-db66c24c4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "NLP (Natural Language Processing) is a field of Artificial Intelligence that teaches computers to understand, \n",
    "interpret, and generate human language (text and speech), enabling applications like voice assistants, translation,\n",
    "chatbots, and spam filters by combining computer science with linguistics to analyze and process natural language data. \n",
    "It allows machines to extract meaning, sentiment, and intent from vast amounts of text and audio, \n",
    "making human-computer interaction more intuitive and automating language-based tasks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b963d03a-dc2b-4f74-a46d-67183cdf5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3cff67-729c-42ee-869d-7f106e4a00aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13388\\3809154357.py:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  re.sub(\"\\d+\",\"\",text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnlp (natural language processing) is a field of artificial intelligence that teaches computers to understand, \\ninterpret, and generate human language (text and speech), enabling applications like voice assistants, translation,\\nchatbots, and spam filters by combining computer science with linguistics to analyze and process natural language data. \\nit allows machines to extract meaning, sentiment, and intent from vast amounts of text and audio, \\nmaking human-computer interaction more intuitive and automating language-based tasks\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove number\n",
    "import re\n",
    "re.sub(\"\\d+\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67437744-d3d5-47a5-9f86-cddaf77142db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b13c3ae-e9a7-4f6c-8dc9-e3af4074c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"\".join([m for m in text if m not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e017d1-3dd1-4c03-a840-d9e9a150e29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnlp natural language processing is a field of artificial intelligence that teaches computers to understand \\ninterpret and generate human language text and speech enabling applications like voice assistants translation\\nchatbots and spam filters by combining computer science with linguistics to analyze and process natural language data \\nit allows machines to extract meaning sentiment and intent from vast amounts of text and audio \\nmaking humancomputer interaction more intuitive and automating languagebased tasks\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bff923a-7573-4f6e-9039-fc7537cdcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50990862-5eaa-432c-846b-db5566f83328",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" \".join([m for m in text.split() if m not in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74824175-2353-4e3b-89c2-67792ae1c5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp natural language processing field artificial intelligence teaches computers understand interpret generate human language text speech enabling applications like voice assistants translation chatbots spam filters combining computer science linguistics analyze process natural language data allows machines extract meaning sentiment intent vast amounts text audio making humancomputer interaction intuitive automating languagebased tasks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3232b0-8fda-4fd2-93ef-0fe1495a6e41",
   "metadata": {},
   "source": [
    "# bags of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d596d52-115e-446a-8233-835898b9d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cont=CountVectorizer()\n",
    "co=cont.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c35894bf-0299-49a5-b775-80c1e45a9daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresNames\n",
      "\n",
      " ['allows' 'amounts' 'analyze' 'applications' 'artificial' 'assistants'\n",
      " 'audio' 'automating' 'chatbots' 'combining' 'computer' 'computers' 'data'\n",
      " 'enabling' 'extract' 'field' 'filters' 'generate' 'human' 'humancomputer'\n",
      " 'intelligence' 'intent' 'interaction' 'interpret' 'intuitive' 'language'\n",
      " 'languagebased' 'like' 'linguistics' 'machines' 'making' 'meaning'\n",
      " 'natural' 'nlp' 'process' 'processing' 'science' 'sentiment' 'spam'\n",
      " 'speech' 'tasks' 'teaches' 'text' 'translation' 'understand' 'vast'\n",
      " 'voice']\n"
     ]
    }
   ],
   "source": [
    "print(\"featuresNames\\n\\n\",cont.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e66af84-68c9-4331-8b8b-672c2b184e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values\n",
      "\n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 2 1 1 1\n",
      "  1 1 1 1 1 1 2 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"values\\n\\n\",co.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a66b0e-d39d-43c9-97f4-1c7cda28a125",
   "metadata": {},
   "source": [
    "# tf-idf (tf =>bags of words )(idf=> inverse documnent frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b26ad56-4964-46ab-9c8b-4a046c1dd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340ab5e0-5fc2-4f30-a699-2f8d7cdfc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "093a19db-b5b1-4b9f-b16f-83af921fed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=t.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cc0bba2-6df6-4840-a17e-4134f531d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresNames\n",
      "\n",
      " ['allows' 'amounts' 'analyze' 'applications' 'artificial' 'assistants'\n",
      " 'audio' 'automating' 'chatbots' 'combining' 'computer' 'computers' 'data'\n",
      " 'enabling' 'extract' 'field' 'filters' 'generate' 'human' 'humancomputer'\n",
      " 'intelligence' 'intent' 'interaction' 'interpret' 'intuitive' 'language'\n",
      " 'languagebased' 'like' 'linguistics' 'machines' 'making' 'meaning'\n",
      " 'natural' 'nlp' 'process' 'processing' 'science' 'sentiment' 'spam'\n",
      " 'speech' 'tasks' 'teaches' 'text' 'translation' 'understand' 'vast'\n",
      " 'voice']\n",
      "values\n",
      "\n",
      " [[0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.38411064 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.25607376 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.25607376 0.12803688 0.12803688 0.12803688 0.12803688]]\n"
     ]
    }
   ],
   "source": [
    "print(\"featuresNames\\n\\n\",t.get_feature_names_out())\n",
    "print(\"values\\n\\n\",tf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd17106-6d9b-4ecd-92c4-f5c8a6713c66",
   "metadata": {},
   "source": [
    "# stemming techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4830f3f4-78c7-41ee-b92e-87d1c8b9952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b07674d9-f054-49e3-ba00-fd84a48951c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "160c5370-257e-4ad3-9a57-48f106178d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing--------> play\n",
      "congratulation--------> congratul\n",
      "buildings--------> build\n",
      "plays--------> play\n",
      "reading--------> read\n"
     ]
    }
   ],
   "source": [
    "words=[\"playing\",\"congratulation\",\"buildings\",\"plays\",\"reading\"]\n",
    "for word in words:\n",
    "    print(word +  \"-------->\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec170920-7342-4a89-a005-19c29afbc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text=\"\".join([stem.stem(word) for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e47f26bc-599d-45a8-8bc7-16fcf18be716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp natural language processing field artificial intelligence teaches computers understand interpret generate human language text speech enabling applications like voice assistants translation chatbots spam filters combining computer science linguistics analyze process natural language data allows machines extract meaning sentiment intent vast amounts text audio making humancomputer interaction intuitive automating languagebased tasks'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fef1c4-22aa-40a9-9b90-b760c8a4de62",
   "metadata": {},
   "source": [
    "# lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6db15e66-45a5-4666-8d29-1fa6152ee828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9431843-5009-46d6-938e-754de9ab4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5def800-04d3-4229-8e9e-7594c848ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2040aafa-a900-4c59-9c53-1f8f42e1e6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp natural language processing field artificial intelligence teaches computers understand interpret generate human language text speech enabling applications like voice assistants translation chatbots spam filters combining computer science linguistics analyze process natural language data allows machines extract meaning sentiment intent vast amounts text audio making humancomputer interaction intuitive automating languagebased tasks'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([lm.lemmatize(word) for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7ce7e4c-db9d-40a9-a083-d62d771d028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing--------> play\n",
      "congratulation--------> congratulation\n",
      "buildings--------> buildings\n",
      "plays--------> play\n",
      "reading--------> read\n",
      "running--------> run\n"
     ]
    }
   ],
   "source": [
    "words=[\"playing\",\"congratulation\",\"buildings\",\"plays\",\"reading\",\"running\"]\n",
    "for word in words:\n",
    "    print(word +  \"-------->\",lm.lemmatize(word,pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35164de1-6ed0-48f0-a5c0-449a44b62d10",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d914b4b-9a24-4395-ac8a-1935f6b3f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (6.33.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b078420-2386-4a89-9759-c2fdbbb70791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "716b1e94-705f-40b3-812b-bc8e9c660cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "211c1623-8130-4290-8980-ed34239f0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12ade0e1-fdf6-426a-8c7b-62c19ab149c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  1,\n",
       "  3,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  2,\n",
       "  1,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  3,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.texts_to_sequences([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f390dd8b-2e58-4572-8af2-83aad54c755e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 1,\n",
       " 'natural': 2,\n",
       " 'text': 3,\n",
       " 'nlp': 4,\n",
       " 'processing': 5,\n",
       " 'field': 6,\n",
       " 'artificial': 7,\n",
       " 'intelligence': 8,\n",
       " 'teaches': 9,\n",
       " 'computers': 10,\n",
       " 'understand': 11,\n",
       " 'interpret': 12,\n",
       " 'generate': 13,\n",
       " 'human': 14,\n",
       " 'speech': 15,\n",
       " 'enabling': 16,\n",
       " 'applications': 17,\n",
       " 'like': 18,\n",
       " 'voice': 19,\n",
       " 'assistants': 20,\n",
       " 'translation': 21,\n",
       " 'chatbots': 22,\n",
       " 'spam': 23,\n",
       " 'filters': 24,\n",
       " 'combining': 25,\n",
       " 'computer': 26,\n",
       " 'science': 27,\n",
       " 'linguistics': 28,\n",
       " 'analyze': 29,\n",
       " 'process': 30,\n",
       " 'data': 31,\n",
       " 'allows': 32,\n",
       " 'machines': 33,\n",
       " 'extract': 34,\n",
       " 'meaning': 35,\n",
       " 'sentiment': 36,\n",
       " 'intent': 37,\n",
       " 'vast': 38,\n",
       " 'amounts': 39,\n",
       " 'audio': 40,\n",
       " 'making': 41,\n",
       " 'humancomputer': 42,\n",
       " 'interaction': 43,\n",
       " 'intuitive': 44,\n",
       " 'automating': 45,\n",
       " 'languagebased': 46,\n",
       " 'tasks': 47}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04122021-b6a9-4ffe-bb4e-51d8518cfbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'field',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'teaches',\n",
       " 'computers',\n",
       " 'understand',\n",
       " 'interpret',\n",
       " 'generate',\n",
       " 'human',\n",
       " 'language',\n",
       " 'text',\n",
       " 'speech',\n",
       " 'enabling',\n",
       " 'applications',\n",
       " 'like',\n",
       " 'voice',\n",
       " 'assistants',\n",
       " 'translation',\n",
       " 'chatbots',\n",
       " 'spam',\n",
       " 'filters',\n",
       " 'combining',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'linguistics',\n",
       " 'analyze',\n",
       " 'process',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'extract',\n",
       " 'meaning',\n",
       " 'sentiment',\n",
       " 'intent',\n",
       " 'vast',\n",
       " 'amounts',\n",
       " 'text',\n",
       " 'audio',\n",
       " 'making',\n",
       " 'humancomputer',\n",
       " 'interaction',\n",
       " 'intuitive',\n",
       " 'automating',\n",
       " 'languagebased',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d5a08-8382-4258-8708-334293ba4e99",
   "metadata": {},
   "source": [
    "# Rnn=>(Recurrent neural netwok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93e8dd-439b-45f4-9dd7-e5036a1c1c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
